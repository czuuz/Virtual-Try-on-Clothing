{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index: 0, Name: person\n",
      "Index: 1, Name: bicycle\n",
      "Index: 2, Name: car\n",
      "Index: 3, Name: motorcycle\n",
      "Index: 4, Name: airplane\n",
      "Index: 5, Name: bus\n",
      "Index: 6, Name: train\n",
      "Index: 7, Name: truck\n",
      "Index: 8, Name: boat\n",
      "Index: 9, Name: traffic light\n",
      "Index: 10, Name: fire hydrant\n",
      "Index: 11, Name: stop sign\n",
      "Index: 12, Name: parking meter\n",
      "Index: 13, Name: bench\n",
      "Index: 14, Name: bird\n",
      "Index: 15, Name: cat\n",
      "Index: 16, Name: dog\n",
      "Index: 17, Name: horse\n",
      "Index: 18, Name: sheep\n",
      "Index: 19, Name: cow\n",
      "Index: 20, Name: elephant\n",
      "Index: 21, Name: bear\n",
      "Index: 22, Name: zebra\n",
      "Index: 23, Name: giraffe\n",
      "Index: 24, Name: backpack\n",
      "Index: 25, Name: umbrella\n",
      "Index: 26, Name: handbag\n",
      "Index: 27, Name: tie\n",
      "Index: 28, Name: suitcase\n",
      "Index: 29, Name: frisbee\n",
      "Index: 30, Name: skis\n",
      "Index: 31, Name: snowboard\n",
      "Index: 32, Name: sports ball\n",
      "Index: 33, Name: kite\n",
      "Index: 34, Name: baseball bat\n",
      "Index: 35, Name: baseball glove\n",
      "Index: 36, Name: skateboard\n",
      "Index: 37, Name: surfboard\n",
      "Index: 38, Name: tennis racket\n",
      "Index: 39, Name: bottle\n",
      "Index: 40, Name: wine glass\n",
      "Index: 41, Name: cup\n",
      "Index: 42, Name: fork\n",
      "Index: 43, Name: knife\n",
      "Index: 44, Name: spoon\n",
      "Index: 45, Name: bowl\n",
      "Index: 46, Name: banana\n",
      "Index: 47, Name: apple\n",
      "Index: 48, Name: sandwich\n",
      "Index: 49, Name: orange\n",
      "Index: 50, Name: broccoli\n",
      "Index: 51, Name: carrot\n",
      "Index: 52, Name: hot dog\n",
      "Index: 53, Name: pizza\n",
      "Index: 54, Name: donut\n",
      "Index: 55, Name: cake\n",
      "Index: 56, Name: chair\n",
      "Index: 57, Name: couch\n",
      "Index: 58, Name: potted plant\n",
      "Index: 59, Name: bed\n",
      "Index: 60, Name: dining table\n",
      "Index: 61, Name: toilet\n",
      "Index: 62, Name: tv\n",
      "Index: 63, Name: laptop\n",
      "Index: 64, Name: mouse\n",
      "Index: 65, Name: remote\n",
      "Index: 66, Name: keyboard\n",
      "Index: 67, Name: cell phone\n",
      "Index: 68, Name: microwave\n",
      "Index: 69, Name: oven\n",
      "Index: 70, Name: toaster\n",
      "Index: 71, Name: sink\n",
      "Index: 72, Name: refrigerator\n",
      "Index: 73, Name: book\n",
      "Index: 74, Name: clock\n",
      "Index: 75, Name: vase\n",
      "Index: 76, Name: scissors\n",
      "Index: 77, Name: teddy bear\n",
      "Index: 78, Name: hair drier\n",
      "Index: 79, Name: toothbrush\n"
     ]
    }
   ],
   "source": [
    "from ultralytics import YOLO\n",
    "\n",
    "# 加载模型\n",
    "yolo_model = YOLO('yolov8m.pt')  # 使用合适的模型权重\n",
    "\n",
    "# 获取模型类别名称\n",
    "class_names = yolo_model.names  # 这是一个字典，通常形式是{index: 'class_name'}\n",
    "\n",
    "# 打印类别名称与索引\n",
    "for index, name in class_names.items():\n",
    "    print(f\"Index: {index}, Name: {name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading https://github.com/ultralytics/assets/releases/download/v8.3.0/yolov8m.pt to 'yolov8m.pt'...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 49.7M/49.7M [31:14<00:00, 27.8kB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing image: O1CN01JHOJMy1V3TMDPGAbH___2665222597.jpg\n",
      "\n",
      "image 1/1 f:\\\\\\\\4878342940501\\\\O1CN01JHOJMy1V3TMDPGAbH___2665222597.jpg: 640x480 1 person, 43.3ms\n",
      "Speed: 3.0ms preprocess, 43.3ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Detected medium confidence clothing boxes: [([4.650688171386719, 19.7235107421875, 1490.79736328125, 1996.7803955078125], 0.39937758445739746)]\n",
      "Largest clothing box area ratio: 0.98\n",
      "Classified as flat clothing\n",
      "\n",
      "Processing image: O1CN016RzuAF1V3TMC2zXdg___2665222597.jpg\n",
      "\n",
      "image 1/1 f:\\\\\\\\4878342940501\\\\O1CN016RzuAF1V3TMC2zXdg___2665222597.jpg: 640x480 (no detections), 26.0ms\n",
      "Speed: 3.0ms preprocess, 26.0ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Classified as not qualified\n",
      "\n",
      "Processing image: O1CN01h4oXtm1V3TMF9U3af___2665222597.jpg\n",
      "\n",
      "image 1/1 f:\\\\\\\\4878342940501\\\\O1CN01h4oXtm1V3TMF9U3af___2665222597.jpg: 640x480 (no detections), 25.0ms\n",
      "Speed: 3.0ms preprocess, 25.0ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Classified as not qualified\n",
      "\n",
      "Processing image: O1CN01JHOJMy1V3TMDPGAbH___2665222597.jpg\n",
      "\n",
      "image 1/1 f:\\\\\\\\4878342940501\\\\O1CN01JHOJMy1V3TMDPGAbH___2665222597.jpg: 640x480 1 person, 26.0ms\n",
      "Speed: 2.0ms preprocess, 26.0ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Detected medium confidence clothing boxes: [([3.98406982421875, 46.526336669921875, 1487.105224609375, 1992.3038330078125], 0.2237069457769394)]\n",
      "Largest clothing box area ratio: 0.96\n",
      "Classified as flat clothing\n",
      "\n",
      "Processing image: O1CN01rDVhTU1V3TMH3mMaA___2665222597.jpg\n",
      "\n",
      "image 1/1 f:\\\\\\\\4878342940501\\\\O1CN01rDVhTU1V3TMH3mMaA___2665222597.jpg: 640x480 1 person, 25.0ms\n",
      "Speed: 2.0ms preprocess, 25.0ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Detected medium confidence clothing boxes: [([3.982830047607422, 4.477691650390625, 1495.373291015625, 1992.719970703125], 0.20325234532356262)]\n",
      "Largest clothing box area ratio: 0.99\n",
      "Classified as flat clothing\n",
      "\n",
      "Processing image: O1CN01RXPjL11V3TM9oCXJD___2665222597.jpg\n",
      "\n",
      "image 1/1 f:\\\\\\\\4878342940501\\\\O1CN01RXPjL11V3TM9oCXJD___2665222597.jpg: 640x480 (no detections), 23.5ms\n",
      "Speed: 2.0ms preprocess, 23.5ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Classified as not qualified\n",
      "\n",
      "Processing image: O1CN01CYtPWu1MUBqQAUK9D___6000000001437-2-tps-2-2.jpg\n",
      "\n",
      "image 1/1 f:\\\\\\\\4878342940501\\\\O1CN01CYtPWu1MUBqQAUK9D___6000000001437-2-tps-2-2.jpg: 640x640 (no detections), 32.0ms\n",
      "Speed: 4.0ms preprocess, 32.0ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Classified as not qualified\n",
      "\n",
      "Processing image: O1CN01gvQs0C1tEVmW1rKu1___3347915870.jpg\n",
      "\n",
      "image 1/1 f:\\\\\\\\4938275336663\\\\O1CN01gvQs0C1tEVmW1rKu1___3347915870.jpg: 640x640 1 person, 30.7ms\n",
      "Speed: 4.0ms preprocess, 30.7ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Detected high confidence person boxes: [([150.4748077392578, 1.431198000907898, 1004.490234375, 1195.7633056640625], 0.8198562264442444)]\n",
      "Largest person box area ratio: 0.71\n",
      "Classified as human clothing\n",
      "\n",
      "Processing image: O1CN011GMNIG1tEVmLckUzb___3347915870.jpg\n",
      "\n",
      "image 1/1 f:\\\\\\\\4938275336663\\\\O1CN011GMNIG1tEVmLckUzb___3347915870.jpg: 640x480 1 person, 20.0ms\n",
      "Speed: 2.0ms preprocess, 20.0ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Detected high confidence person boxes: [([1.42242431640625, 2.9888916015625, 1140.6036376953125, 1593.7664794921875], 0.8523613810539246)]\n",
      "Largest person box area ratio: 0.94\n",
      "Classified as human clothing\n",
      "\n",
      "Processing image: O1CN01iBHvHB1tEVlrf8fbo___3347915870.jpg\n",
      "\n",
      "image 1/1 f:\\\\\\\\4938275336663\\\\O1CN01iBHvHB1tEVlrf8fbo___3347915870.jpg: 640x480 (no detections), 19.0ms\n",
      "Speed: 2.0ms preprocess, 19.0ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Classified as not qualified\n",
      "\n",
      "Processing image: O1CN01ltMFE51tEVmLchcBL___3347915870.jpg\n",
      "\n",
      "image 1/1 f:\\\\\\\\4938275336663\\\\O1CN01ltMFE51tEVmLchcBL___3347915870.jpg: 640x480 1 person, 18.0ms\n",
      "Speed: 3.0ms preprocess, 18.0ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Detected high confidence person boxes: [([1.0986328125, 6.124420166015625, 1181.173583984375, 1595.3016357421875], 0.9109408259391785)]\n",
      "Largest person box area ratio: 0.98\n",
      "Classified as human clothing\n",
      "\n",
      "Processing image: O1CN01Qvaqk61tEVlrf8CUO___3347915870.jpg\n",
      "\n",
      "image 1/1 f:\\\\\\\\4938275336663\\\\O1CN01Qvaqk61tEVlrf8CUO___3347915870.jpg: 640x480 (no detections), 18.0ms\n",
      "Speed: 3.0ms preprocess, 18.0ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Classified as not qualified\n",
      "\n",
      "Processing image: O1CN01yeB5Bm1tEVmUJIdXO___3347915870.jpg\n",
      "\n",
      "image 1/1 f:\\\\\\\\4938275336663\\\\O1CN01yeB5Bm1tEVmUJIdXO___3347915870.jpg: 640x480 1 person, 18.5ms\n",
      "Speed: 2.0ms preprocess, 18.5ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Detected high confidence person boxes: [([1.15753173828125, 3.83453369140625, 1198.919189453125, 1596.692138671875], 0.8930596709251404)]\n",
      "Largest person box area ratio: 0.99\n",
      "Classified as human clothing\n",
      "\n",
      "Processing image: O1CN01gMzl2m1tEVpgStVFG___3347915870.jpg\n",
      "\n",
      "image 1/1 f:\\\\\\\\4938275336663\\\\O1CN01gMzl2m1tEVpgStVFG___3347915870.jpg: 640x640 1 person, 26.0ms\n",
      "Speed: 8.0ms preprocess, 26.0ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Detected high confidence person boxes: [([226.2298583984375, 99.00474548339844, 655.6176147460938, 798.2369995117188], 0.9630100131034851)]\n",
      "Largest person box area ratio: 0.47\n",
      "Classified as not qualified\n",
      "\n",
      "Processing image: O1CN01gvQs0C1tEVmW1rKu1___3347915870.jpg\n",
      "\n",
      "image 1/1 f:\\\\\\\\4938275336663\\\\O1CN01gvQs0C1tEVmW1rKu1___3347915870.jpg: 640x640 1 person, 24.3ms\n",
      "Speed: 6.0ms preprocess, 24.3ms inference, 3.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Detected high confidence person boxes: [([148.24755859375, 0.9622191786766052, 1004.0258178710938, 1194.8468017578125], 0.8229172825813293)]\n",
      "Largest person box area ratio: 0.71\n",
      "Classified as human clothing\n",
      "\n",
      "Processing image: O1CN01kiXyrQ1tEVlwoiXNG___3347915870.jpg\n",
      "\n",
      "image 1/1 f:\\\\\\\\4938275336663\\\\O1CN01kiXyrQ1tEVlwoiXNG___3347915870.jpg: 640x640 (no detections), 24.0ms\n",
      "Speed: 4.0ms preprocess, 24.0ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Classified as not qualified\n",
      "\n",
      "Processing image: O1CN01pn4ty71tEVpkmXblE___3347915870.jpg\n",
      "\n",
      "image 1/1 f:\\\\\\\\4938275336663\\\\O1CN01pn4ty71tEVpkmXblE___3347915870.jpg: 640x640 1 person, 24.5ms\n",
      "Speed: 4.0ms preprocess, 24.5ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Detected high confidence person boxes: [([218.29454040527344, 102.35755920410156, 650.06396484375, 799.1502685546875], 0.9597195386886597)]\n",
      "Largest person box area ratio: 0.47\n",
      "Classified as not qualified\n",
      "\n",
      "Processing image: O1CN01HVuXFx1Dkpr40CUfr___2671610255.jpg\n",
      "\n",
      "image 1/1 f:\\\\\\\\5046737240504\\\\O1CN01HVuXFx1Dkpr40CUfr___2671610255.jpg: 640x640 (no detections), 25.0ms\n",
      "Speed: 5.0ms preprocess, 25.0ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Classified as not qualified\n",
      "\n",
      "Processing image: O1CN015lKjNk1Dkpqx9HJsj___2671610255.jpg\n",
      "\n",
      "image 1/1 f:\\\\\\\\5046737240504\\\\O1CN015lKjNk1Dkpqx9HJsj___2671610255.jpg: 640x480 (no detections), 19.0ms\n",
      "Speed: 3.0ms preprocess, 19.0ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Classified as not qualified\n",
      "\n",
      "Processing image: O1CN01qlP1we1DkpqzX08Yd___2671610255.jpg\n",
      "\n",
      "image 1/1 f:\\\\\\\\5046737240504\\\\O1CN01qlP1we1DkpqzX08Yd___2671610255.jpg: 640x480 (no detections), 27.0ms\n",
      "Speed: 4.0ms preprocess, 27.0ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Classified as not qualified\n",
      "\n",
      "Processing image: O1CN01rFS5pF1Dkpr0Al301___2671610255.jpg\n",
      "\n",
      "image 1/1 f:\\\\\\\\5046737240504\\\\O1CN01rFS5pF1Dkpr0Al301___2671610255.jpg: 640x480 1 person, 18.1ms\n",
      "Speed: 2.0ms preprocess, 18.1ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Detected high confidence person boxes: [([20.462894439697266, 205.1255645751953, 726.9544677734375, 732.275634765625], 0.8058605790138245)]\n",
      "Largest person box area ratio: 0.50\n",
      "Classified as not qualified\n",
      "\n",
      "Processing image: O1CN01RjPqhG1Dkpr0fyCY3___2671610255.jpg\n",
      "\n",
      "image 1/1 f:\\\\\\\\5046737240504\\\\O1CN01RjPqhG1Dkpr0fyCY3___2671610255.jpg: 640x480 (no detections), 18.0ms\n",
      "Speed: 2.0ms preprocess, 18.0ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Classified as not qualified\n",
      "\n",
      "Processing image: O1CN01UjRCHc1DkpsNseLD0___2671610255.jpg\n",
      "\n",
      "image 1/1 f:\\\\\\\\5046737240504\\\\O1CN01UjRCHc1DkpsNseLD0___2671610255.jpg: 640x480 (no detections), 20.0ms\n",
      "Speed: 3.0ms preprocess, 20.0ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Classified as not qualified\n",
      "\n",
      "Processing image: O1CN01beS7tD1DkpqoJiFPx___2671610255.jpg\n",
      "\n",
      "image 1/1 f:\\\\\\\\5046737240504\\\\O1CN01beS7tD1DkpqoJiFPx___2671610255.jpg: 640x640 (no detections), 27.0ms\n",
      "Speed: 7.0ms preprocess, 27.0ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Classified as not qualified\n",
      "\n",
      "Processing image: O1CN01CYtPWu1MUBqQAUK9D___6000000001437-2-tps-2-2.jpg\n",
      "\n",
      "image 1/1 f:\\\\\\\\5046737240504\\\\O1CN01CYtPWu1MUBqQAUK9D___6000000001437-2-tps-2-2.jpg: 640x640 (no detections), 25.0ms\n",
      "Speed: 5.0ms preprocess, 25.0ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Classified as not qualified\n",
      "\n",
      "Processing image: O1CN01dXHwf51DkptCJ7WCp___2671610255.jpg\n",
      "\n",
      "image 1/1 f:\\\\\\\\5046737240504\\\\O1CN01dXHwf51DkptCJ7WCp___2671610255.jpg: 640x640 (no detections), 26.0ms\n",
      "Speed: 4.0ms preprocess, 26.0ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Classified as not qualified\n",
      "\n",
      "Processing image: O1CN01fLNOr01Dkpr0eSDJi___2671610255.jpg\n",
      "\n",
      "image 1/1 f:\\\\\\\\5046737240504\\\\O1CN01fLNOr01Dkpr0eSDJi___2671610255.jpg: 640x640 (no detections), 24.0ms\n",
      "Speed: 4.0ms preprocess, 24.0ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Classified as not qualified\n",
      "\n",
      "Processing image: O1CN01HVuXFx1Dkpr40CUfr___2671610255.jpg\n",
      "\n",
      "image 1/1 f:\\\\\\\\5046737240504\\\\O1CN01HVuXFx1Dkpr40CUfr___2671610255.jpg: 640x640 (no detections), 25.6ms\n",
      "Speed: 4.0ms preprocess, 25.6ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Classified as not qualified\n",
      "\n",
      "Processing image: O1CN01ulpk4n1DkptGEvib6___2671610255.jpg\n",
      "\n",
      "image 1/1 f:\\\\\\\\5046737240504\\\\O1CN01ulpk4n1DkptGEvib6___2671610255.jpg: 640x640 (no detections), 24.0ms\n",
      "Speed: 4.0ms preprocess, 24.0ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Classified as not qualified\n",
      "\n",
      "Processing image: O1CN01czPb591mHzNiAEH8h___914084930.jpg\n",
      "\n",
      "image 1/1 f:\\\\\\\\5054208906304\\\\O1CN01czPb591mHzNiAEH8h___914084930.jpg: 640x640 1 person, 24.0ms\n",
      "Speed: 5.0ms preprocess, 24.0ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Detected high confidence person boxes: [([194.65538024902344, 79.78579711914062, 956.5205688476562, 1197.006591796875], 0.9629630446434021)]\n",
      "Largest person box area ratio: 0.59\n",
      "Classified as human clothing\n",
      "\n",
      "Processing image: O1CN01czPb591mHzNiAEH8h___914084930.jpg\n",
      "\n",
      "image 1/1 f:\\\\\\\\5054208906304\\\\O1CN01czPb591mHzNiAEH8h___914084930.jpg: 640x640 1 person, 24.1ms\n",
      "Speed: 5.0ms preprocess, 24.1ms inference, 3.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Detected high confidence person boxes: [([200.1070556640625, 76.05330657958984, 954.2760620117188, 1196.50537109375], 0.9605718851089478)]\n",
      "Largest person box area ratio: 0.59\n",
      "Classified as human clothing\n",
      "\n",
      "Processing image: O1CN01ixRiD91mHzNeVxvgN___914084930.jpg\n",
      "\n",
      "image 1/1 f:\\\\\\\\5054208906304\\\\O1CN01ixRiD91mHzNeVxvgN___914084930.jpg: 640x640 (no detections), 24.0ms\n",
      "Speed: 5.0ms preprocess, 24.0ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Classified as not qualified\n",
      "\n",
      "Processing image: O1CN01jBpoYP1mHzNgGHn4G___914084930.jpg\n",
      "\n",
      "image 1/1 f:\\\\\\\\5054208906304\\\\O1CN01jBpoYP1mHzNgGHn4G___914084930.jpg: 640x640 3 persons, 24.3ms\n",
      "Speed: 5.0ms preprocess, 24.3ms inference, 3.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Detected high confidence person boxes: [([65.7515640258789, 347.1865234375, 471.97802734375, 1464.4285888671875], 0.951695442199707), ([1021.1187744140625, 344.46502685546875, 1436.50732421875, 1466.45703125], 0.9509680271148682), ([617.0515747070312, 343.2909851074219, 869.2418823242188, 1461.5758056640625], 0.8641945719718933)]\n",
      "Largest person box area ratio: 0.21\n",
      "Classified as not qualified\n",
      "\n",
      "Processing image: O1CN01pFzZBA1mHzNjQHj3W___914084930.jpg\n",
      "\n",
      "image 1/1 f:\\\\\\\\5054208906304\\\\O1CN01pFzZBA1mHzNjQHj3W___914084930.jpg: 640x640 1 person, 24.3ms\n",
      "Speed: 4.0ms preprocess, 24.3ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Detected high confidence person boxes: [([273.2011413574219, 86.9185562133789, 977.287841796875, 1197.9095458984375], 0.8878421783447266)]\n",
      "Largest person box area ratio: 0.54\n",
      "Classified as human clothing\n",
      "\n",
      "Processing image: O1CN01WziDSg1mHzNfRddvz___914084930.jpg\n",
      "\n",
      "image 1/1 f:\\\\\\\\5054208906304\\\\O1CN01WziDSg1mHzNfRddvz___914084930.jpg: 640x640 1 person, 24.0ms\n",
      "Speed: 5.0ms preprocess, 24.0ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Detected high confidence person boxes: [([258.52532958984375, 84.82337188720703, 979.1556396484375, 1200.0], 0.9081611037254333)]\n",
      "Largest person box area ratio: 0.56\n",
      "Classified as human clothing\n",
      "\n",
      "Processing image: O1CN01czPb591mHzNiAEH8h___914084930.jpg\n",
      "\n",
      "image 1/1 f:\\\\\\\\5054208906304\\\\O1CN01czPb591mHzNiAEH8h___914084930.jpg: 640x640 1 person, 24.0ms\n",
      "Speed: 4.5ms preprocess, 24.0ms inference, 3.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Detected high confidence person boxes: [([188.97799682617188, 76.32098388671875, 955.9666748046875, 1197.1754150390625], 0.9606033563613892)]\n",
      "Largest person box area ratio: 0.60\n",
      "Classified as human clothing\n",
      "\n",
      "Processing image: O1CN01pFzZBA1mHzNjQHj3W___914084930.jpg\n",
      "\n",
      "image 1/1 f:\\\\\\\\5054208906304\\\\O1CN01pFzZBA1mHzNjQHj3W___914084930.jpg: 640x640 1 person, 25.0ms\n",
      "Speed: 4.0ms preprocess, 25.0ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Detected high confidence person boxes: [([274.6580810546875, 57.57796859741211, 978.00048828125, 1196.8564453125], 0.8690216541290283)]\n",
      "Largest person box area ratio: 0.56\n",
      "Classified as human clothing\n",
      "\n",
      "Processing image: O1CN01ScrvH61DkprJ1NSvN___2671610255.jpg\n",
      "\n",
      "image 1/1 f:\\\\\\\\5059916592895\\\\O1CN01ScrvH61DkprJ1NSvN___2671610255.jpg: 640x640 1 person, 25.0ms\n",
      "Speed: 5.0ms preprocess, 25.0ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Detected medium confidence clothing boxes: [([68.4326171875, 99.72671508789062, 739.1769409179688, 647.618896484375], 0.7020412683486938)]\n",
      "Largest clothing box area ratio: 0.57\n",
      "Classified as flat clothing\n",
      "\n",
      "Processing image: O1CN0145uXdG1DkprJDMiRa___2671610255.jpg\n",
      "\n",
      "image 1/1 f:\\\\\\\\5059916592895\\\\O1CN0145uXdG1DkprJDMiRa___2671610255.jpg: 640x480 1 person, 20.0ms\n",
      "Speed: 3.0ms preprocess, 20.0ms inference, 3.0ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Detected medium confidence clothing boxes: [([53.034828186035156, 224.30267333984375, 691.5118408203125, 741.7460327148438], 0.3437645733356476)]\n",
      "Largest clothing box area ratio: 0.44\n",
      "Classified as not qualified\n",
      "\n",
      "Processing image: O1CN01dhis9u1DkprHqdRBe___2671610255.jpg\n",
      "\n",
      "image 1/1 f:\\\\\\\\5059916592895\\\\O1CN01dhis9u1DkprHqdRBe___2671610255.jpg: 640x480 (no detections), 19.0ms\n",
      "Speed: 2.0ms preprocess, 19.0ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Classified as not qualified\n",
      "\n",
      "Processing image: O1CN01gj08kM1DkpsR856tg___2671610255.jpg\n",
      "\n",
      "image 1/1 f:\\\\\\\\5059916592895\\\\O1CN01gj08kM1DkpsR856tg___2671610255.jpg: 640x480 1 person, 18.0ms\n",
      "Speed: 2.0ms preprocess, 18.0ms inference, 3.0ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Detected medium confidence clothing boxes: [([56.98833465576172, 224.9434051513672, 691.544921875, 741.9164428710938], 0.4248265027999878)]\n",
      "Largest clothing box area ratio: 0.44\n",
      "Classified as not qualified\n",
      "\n",
      "Processing image: O1CN01sdXNm01DkprMwOOHY___2671610255.jpg\n",
      "\n",
      "image 1/1 f:\\\\\\\\5059916592895\\\\O1CN01sdXNm01DkprMwOOHY___2671610255.jpg: 640x480 1 person, 18.1ms\n",
      "Speed: 2.0ms preprocess, 18.1ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Detected medium confidence clothing boxes: [([32.3624382019043, 512.9540405273438, 359.7510681152344, 935.8766479492188], 0.25332000851631165)]\n",
      "Largest clothing box area ratio: 0.18\n",
      "Classified as not qualified\n",
      "\n",
      "Processing image: O1CN01XNWzxJ1DkprKK22uf___2671610255.jpg\n",
      "\n",
      "image 1/1 f:\\\\\\\\5059916592895\\\\O1CN01XNWzxJ1DkprKK22uf___2671610255.jpg: 640x480 1 person, 19.0ms\n",
      "Speed: 2.0ms preprocess, 19.0ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Detected medium confidence clothing boxes: [([43.975067138671875, 184.25579833984375, 716.2529907226562, 732.013916015625], 0.2540523409843445)]\n",
      "Largest clothing box area ratio: 0.49\n",
      "Classified as not qualified\n",
      "\n",
      "Processing image: O1CN01ir9V1V1DkprF511wO___2671610255.jpg\n",
      "\n",
      "image 1/1 f:\\\\\\\\5059916592895\\\\O1CN01ir9V1V1DkprF511wO___2671610255.jpg: 640x640 (no detections), 25.5ms\n",
      "Speed: 5.0ms preprocess, 25.5ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Classified as not qualified\n",
      "\n",
      "Processing image: O1CN01ScrvH61DkprJ1NSvN___2671610255.jpg\n",
      "\n",
      "image 1/1 f:\\\\\\\\5059916592895\\\\O1CN01ScrvH61DkprJ1NSvN___2671610255.jpg: 640x640 (no detections), 26.0ms\n",
      "Speed: 4.0ms preprocess, 26.0ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Classified as not qualified\n",
      "\n",
      "Processing image: O1CN01OQzplZ1SNg5ZWvgzM___2204159672235.jpg\n",
      "\n",
      "image 1/1 f:\\\\\\\\5113838617150\\\\O1CN01OQzplZ1SNg5ZWvgzM___2204159672235.jpg: 640x640 (no detections), 24.6ms\n",
      "Speed: 3.9ms preprocess, 24.6ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Classified as not qualified\n",
      "\n",
      "Processing image: O1CN01hKbD8Y1SNg4Lx6KLU___2204159672235.jpg\n",
      "\n",
      "image 1/1 f:\\\\\\\\5113838617150\\\\O1CN01hKbD8Y1SNg4Lx6KLU___2204159672235.jpg: 640x480 (no detections), 20.0ms\n",
      "Speed: 3.0ms preprocess, 20.0ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Classified as not qualified\n",
      "\n",
      "Processing image: O1CN01ip8Srs1SNg4I5xUIo___2204159672235.jpg\n",
      "\n",
      "image 1/1 f:\\\\\\\\5113838617150\\\\O1CN01ip8Srs1SNg4I5xUIo___2204159672235.jpg: 640x480 2 persons, 19.0ms\n",
      "Speed: 2.0ms preprocess, 19.0ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Detected high confidence person boxes: [([34.91341018676758, 111.444091796875, 391.197265625, 998.3759155273438], 0.9243701100349426), ([363.3282165527344, 165.16171264648438, 696.825439453125, 996.9358520507812], 0.8842812776565552)]\n",
      "Largest person box area ratio: 0.42\n",
      "Classified as not qualified\n",
      "\n",
      "Processing image: O1CN01LiXQ3g1SNg4M0Y1AF___0-item_pic.jpg\n",
      "\n",
      "image 1/1 f:\\\\\\\\5113838617150\\\\O1CN01LiXQ3g1SNg4M0Y1AF___0-item_pic.jpg: 640x480 (no detections), 19.0ms\n",
      "Speed: 2.0ms preprocess, 19.0ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Classified as not qualified\n",
      "\n",
      "Processing image: O1CN01tdpUjg1SNg4Lx67qo___2204159672235.jpg\n",
      "\n",
      "image 1/1 f:\\\\\\\\5113838617150\\\\O1CN01tdpUjg1SNg4Lx67qo___2204159672235.jpg: 640x480 (no detections), 25.0ms\n",
      "Speed: 4.0ms preprocess, 25.0ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Classified as not qualified\n",
      "\n",
      "Processing image: O1CN01VdIfmU1SNg4Qdrsud___2204159672235.jpg\n",
      "\n",
      "image 1/1 f:\\\\\\\\5113838617150\\\\O1CN01VdIfmU1SNg4Qdrsud___2204159672235.jpg: 640x480 (no detections), 19.0ms\n",
      "Speed: 3.0ms preprocess, 19.0ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Classified as not qualified\n",
      "\n",
      "Processing image: O1CN01elQiOc1SNg4NLKmuJ___2204159672235.jpg\n",
      "\n",
      "image 1/1 f:\\\\\\\\5113838617150\\\\O1CN01elQiOc1SNg4NLKmuJ___2204159672235.jpg: 640x640 (no detections), 26.0ms\n",
      "Speed: 5.0ms preprocess, 26.0ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Classified as not qualified\n",
      "\n",
      "Processing image: O1CN01OQzplZ1SNg5ZWvgzM___2204159672235.jpg\n",
      "\n",
      "image 1/1 f:\\\\\\\\5113838617150\\\\O1CN01OQzplZ1SNg5ZWvgzM___2204159672235.jpg: 640x640 (no detections), 26.0ms\n",
      "Speed: 5.0ms preprocess, 26.0ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Classified as not qualified\n",
      "\n",
      "Processing image: O1CN01qN9Bdm1XQx1zzzjKR___2215482312919.jpg\n",
      "\n",
      "image 1/1 f:\\\\\\\\5137174088548\\\\O1CN01qN9Bdm1XQx1zzzjKR___2215482312919.jpg: 640x480 1 person, 19.0ms\n",
      "Speed: 3.0ms preprocess, 19.0ms inference, 3.0ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Detected medium confidence clothing boxes: [([438.2137451171875, 450.3680725097656, 2495.37158203125, 4015.520751953125], 0.5293128490447998)]\n",
      "Largest clothing box area ratio: 0.60\n",
      "Classified as flat clothing\n",
      "\n",
      "Processing image: O1CN010K6WEz1XQx0U5JXte___2215482312919.jpg\n",
      "\n",
      "image 1/1 f:\\\\\\\\5137174088548\\\\O1CN010K6WEz1XQx0U5JXte___2215482312919.jpg: 640x416 2 persons, 40.5ms\n",
      "Speed: 3.0ms preprocess, 40.5ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 416)\n",
      "Detected high confidence person boxes: [([188.6271209716797, 2.596062660217285, 1110.172607421875, 1826.998046875], 0.9506114721298218)]\n",
      "Largest person box area ratio: 0.79\n",
      "Classified as human clothing\n",
      "\n",
      "Processing image: O1CN010tdxgR1XQx0T4SrAC___2215482312919.jpg\n",
      "\n",
      "image 1/1 f:\\\\\\\\5137174088548\\\\O1CN010tdxgR1XQx0T4SrAC___2215482312919.jpg: 640x416 2 persons, 18.0ms\n",
      "Speed: 2.0ms preprocess, 18.0ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 416)\n",
      "Detected high confidence person boxes: [([372.4720153808594, 1.2108112573623657, 1157.513671875, 1788.0281982421875], 0.943789005279541)]\n",
      "Largest person box area ratio: 0.63\n",
      "Classified as human clothing\n",
      "\n",
      "Processing image: O1CN0129XUKq1XQx0U5IL3E___2215482312919.jpg\n",
      "\n",
      "image 1/1 f:\\\\\\\\5137174088548\\\\O1CN0129XUKq1XQx0U5IL3E___2215482312919.jpg: 640x480 2 persons, 19.0ms\n",
      "Speed: 4.0ms preprocess, 19.0ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Detected medium confidence clothing boxes: [([2344.225341796875, 230.8863983154297, 2797.989501953125, 904.5059204101562], 0.6980420351028442), ([3.600659132003784, 1699.6195068359375, 2449.30126953125, 3701.563720703125], 0.3556150197982788)]\n",
      "Largest clothing box area ratio: 0.40\n",
      "Classified as not qualified\n",
      "\n",
      "Processing image: O1CN01puD5G01XQx0W7Lzwu___2215482312919.jpg\n",
      "\n",
      "image 1/1 f:\\\\\\\\5137174088548\\\\O1CN01puD5G01XQx0W7Lzwu___2215482312919.jpg: 640x480 (no detections), 18.0ms\n",
      "Speed: 3.0ms preprocess, 18.0ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Classified as not qualified\n",
      "\n",
      "Processing image: O1CN01Y2ponG1XQx0cMbDBq___2215482312919.jpg\n",
      "\n",
      "image 1/1 f:\\\\\\\\5137174088548\\\\O1CN01Y2ponG1XQx0cMbDBq___2215482312919.jpg: 640x640 3 persons, 27.0ms\n",
      "Speed: 5.0ms preprocess, 27.0ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Detected high confidence person boxes: [([240.89395141601562, 359.0478515625, 915.4033203125, 1899.232421875], 0.9371165037155151), ([1520.6890869140625, 1314.089111328125, 2024.642578125, 2046.2652587890625], 0.9228612184524536), ([1648.9945068359375, 8.812158584594727, 1994.9000244140625, 819.1408081054688], 0.8777945041656494)]\n",
      "Largest person box area ratio: 0.25\n",
      "Classified as not qualified\n",
      "\n",
      "Processing image: O1CN01CYtPWu1MUBqQAUK9D___6000000001437-2-tps-2-2.jpg\n",
      "\n",
      "image 1/1 f:\\\\\\\\5137174088548\\\\O1CN01CYtPWu1MUBqQAUK9D___6000000001437-2-tps-2-2.jpg: 640x640 (no detections), 24.5ms\n",
      "Speed: 4.0ms preprocess, 24.5ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Classified as not qualified\n",
      "\n",
      "Processing image: O1CN01cV1nmu1Ry1tBtBlMZ___251602179.jpg\n",
      "\n",
      "image 1/1 f:\\\\\\\\5232184654600\\\\O1CN01cV1nmu1Ry1tBtBlMZ___251602179.jpg: 640x640 1 person, 25.0ms\n",
      "Speed: 6.0ms preprocess, 25.0ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Detected medium confidence clothing boxes: [([24.383316040039062, 90.11146545410156, 780.3604736328125, 695.1024169921875], 0.5743096470832825)]\n",
      "Largest clothing box area ratio: 0.71\n",
      "Classified as flat clothing\n",
      "\n",
      "Processing image: O1CN01aqfGAs1Ry1tBvLZ46___251602179.jpg\n",
      "\n",
      "image 1/1 f:\\\\\\\\5232184654600\\\\O1CN01aqfGAs1Ry1tBvLZ46___251602179.jpg: 640x640 1 person, 24.9ms\n",
      "Speed: 5.0ms preprocess, 24.9ms inference, 3.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Detected medium confidence clothing boxes: [([25.42236328125, 88.23165893554688, 764.4671630859375, 694.9700927734375], 0.7817341685295105)]\n",
      "Largest clothing box area ratio: 0.70\n",
      "Classified as flat clothing\n",
      "\n",
      "Processing image: O1CN01bgj2B71Ry1tF47JXX___251602179.jpg\n",
      "\n",
      "image 1/1 f:\\\\\\\\5232184654600\\\\O1CN01bgj2B71Ry1tF47JXX___251602179.jpg: 640x640 1 person, 25.0ms\n",
      "Speed: 5.0ms preprocess, 25.0ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Detected medium confidence clothing boxes: [([65.39512634277344, 90.78594207763672, 730.101806640625, 639.3836059570312], 0.6247746348381042)]\n",
      "Largest clothing box area ratio: 0.57\n",
      "Classified as flat clothing\n",
      "\n",
      "Processing image: O1CN01by8TFk1Ry1tIp7mRP___251602179.jpg\n",
      "\n",
      "image 1/1 f:\\\\\\\\5232184654600\\\\O1CN01by8TFk1Ry1tIp7mRP___251602179.jpg: 640x640 1 person, 24.0ms\n",
      "Speed: 4.0ms preprocess, 24.0ms inference, 3.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Detected medium confidence clothing boxes: [([62.180633544921875, 85.76469421386719, 736.2271118164062, 635.07861328125], 0.4378138482570648)]\n",
      "Largest clothing box area ratio: 0.58\n",
      "Classified as flat clothing\n",
      "\n",
      "Processing image: O1CN01M2JmnP1Ry1tBvIXwm___251602179.jpg\n",
      "\n",
      "image 1/1 f:\\\\\\\\5232184654600\\\\O1CN01M2JmnP1Ry1tBvIXwm___251602179.jpg: 640x640 3 persons, 25.0ms\n",
      "Speed: 4.0ms preprocess, 25.0ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Detected medium confidence clothing boxes: [([48.056793212890625, 155.1068115234375, 541.9912719726562, 553.3125], 0.7716085314750671), ([400.35498046875, 155.60830688476562, 753.4003295898438, 554.0564575195312], 0.6482946276664734), ([642.455078125, 307.4024963378906, 658.0121459960938, 332.5381164550781], 0.2495572715997696)]\n",
      "Largest clothing box area ratio: 0.31\n",
      "Classified as not qualified\n",
      "\n",
      "Processing image: O1CN01mB6a0b1Ry1tBvLISN___251602179.jpg\n",
      "\n",
      "image 1/1 f:\\\\\\\\5232184654600\\\\O1CN01mB6a0b1Ry1tBvLISN___251602179.jpg: 640x640 1 person, 24.0ms\n",
      "Speed: 5.0ms preprocess, 24.0ms inference, 3.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Detected medium confidence clothing boxes: [([26.73675537109375, 428.327880859375, 785.59521484375, 788.1834716796875], 0.27485010027885437)]\n",
      "Largest clothing box area ratio: 0.43\n",
      "Classified as not qualified\n",
      "\n",
      "Processing image: O1CN01CYtPWu1MUBqQAUK9D___6000000001437-2-tps-2-2.jpg\n",
      "\n",
      "image 1/1 f:\\\\\\\\5232184654600\\\\O1CN01CYtPWu1MUBqQAUK9D___6000000001437-2-tps-2-2.jpg: 640x640 (no detections), 24.7ms\n",
      "Speed: 4.0ms preprocess, 24.7ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Classified as not qualified\n",
      "\n",
      "Processing image: O1CN017E3fzY1pL93MX2t83___2213636395343.jpg\n",
      "\n",
      "image 1/1 f:\\\\\\\\5259494574380\\\\O1CN017E3fzY1pL93MX2t83___2213636395343.jpg: 640x640 1 person, 23.5ms\n",
      "Speed: 5.0ms preprocess, 23.5ms inference, 3.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Detected medium confidence clothing boxes: [([1100.8673095703125, 1820.9903564453125, 1320.8035888671875, 1999.314453125], 0.3349711298942566)]\n",
      "Largest clothing box area ratio: 0.01\n",
      "Classified as not qualified\n",
      "\n",
      "Processing image: O1CN019DKB2J1pL93YkyFYT___2213636395343.jpg\n",
      "\n",
      "image 1/1 f:\\\\\\\\5259494574380\\\\O1CN019DKB2J1pL93YkyFYT___2213636395343.jpg: 640x480 2 persons, 20.0ms\n",
      "Speed: 2.0ms preprocess, 20.0ms inference, 3.0ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Detected high confidence person boxes: [([1001.758056640625, 481.76531982421875, 1523.630859375, 2576.907470703125], 0.9285013675689697), ([342.2499694824219, 361.35443115234375, 968.6256103515625, 2591.0], 0.9211609363555908)]\n",
      "Largest person box area ratio: 0.28\n",
      "Classified as not qualified\n",
      "\n",
      "Processing image: O1CN01JBFtRs1pL93CouhwV___2213636395343.jpg\n",
      "\n",
      "image 1/1 f:\\\\\\\\5259494574380\\\\O1CN01JBFtRs1pL93CouhwV___2213636395343.jpg: 640x480 1 person, 19.0ms\n",
      "Speed: 2.0ms preprocess, 19.0ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Detected high confidence person boxes: [([509.8438720703125, 67.45691680908203, 1090.2545166015625, 1931.2373046875], 0.9440908432006836)]\n",
      "Largest person box area ratio: 0.39\n",
      "Classified as not qualified\n",
      "\n",
      "Processing image: O1CN01Mevn2I1pL93ZnOPjW___2213636395343.jpg\n",
      "\n",
      "image 1/1 f:\\\\\\\\5259494574380\\\\O1CN01Mevn2I1pL93ZnOPjW___2213636395343.jpg: 640x480 2 persons, 18.0ms\n",
      "Speed: 2.0ms preprocess, 18.0ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Detected high confidence person boxes: [([0.6468005180358887, 363.78363037109375, 1373.6822509765625, 3689.5556640625], 0.9609811305999756), ([1270.076904296875, 932.457763671875, 2471.90869140625, 3689.85302734375], 0.9329047799110413)]\n",
      "Largest person box area ratio: 0.45\n",
      "Classified as not qualified\n",
      "\n",
      "Processing image: O1CN01w4pnM81pL93MXiAy7___2213636395343.jpg\n",
      "\n",
      "image 1/1 f:\\\\\\\\5259494574380\\\\O1CN01w4pnM81pL93MXiAy7___2213636395343.jpg: 640x480 1 person, 18.0ms\n",
      "Speed: 2.0ms preprocess, 18.0ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Detected high confidence person boxes: [([325.1818542480469, 297.7029113769531, 697.3612060546875, 1196.616455078125], 0.9331369400024414)]\n",
      "Largest person box area ratio: 0.31\n",
      "Classified as not qualified\n",
      "\n",
      "Processing image: O1CN01wuTg0e1pL93LHq7ra___2213636395343.jpg\n",
      "\n",
      "image 1/1 f:\\\\\\\\5259494574380\\\\O1CN01wuTg0e1pL93LHq7ra___2213636395343.jpg: 640x480 1 person, 19.0ms\n",
      "Speed: 2.0ms preprocess, 19.0ms inference, 3.0ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Detected high confidence person boxes: [([184.5648651123047, 522.3831787109375, 746.9418334960938, 1199.181640625], 0.9617867469787598)]\n",
      "Largest person box area ratio: 0.35\n",
      "Classified as not qualified\n",
      "\n",
      "Processing image: O1CN01CYtPWu1MUBqQAUK9D___6000000001437-2-tps-2-2.jpg\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 118\u001b[0m\n\u001b[0;32m    116\u001b[0m base_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m夹克\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    117\u001b[0m output_base_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m夹克1\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m--> 118\u001b[0m \u001b[43mdetect_and_classify_images\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbase_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_base_path\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[2], line 64\u001b[0m, in \u001b[0;36mdetect_and_classify_images\u001b[1;34m(base_path, output_base_path)\u001b[0m\n\u001b[0;32m     61\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mProcessing image: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mos\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mbasename(image_path)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     63\u001b[0m \u001b[38;5;66;03m# 使用修改的置信度阈值进行探测\u001b[39;00m\n\u001b[1;32m---> 64\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43myolo_model_person\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclasses\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcuda\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.2\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     66\u001b[0m high_confidence_boxes \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m     67\u001b[0m medium_confidence_boxes \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[1;32md:\\Python\\Lib\\site-packages\\ultralytics\\engine\\model.py:176\u001b[0m, in \u001b[0;36mModel.__call__\u001b[1;34m(self, source, stream, **kwargs)\u001b[0m\n\u001b[0;32m    147\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\n\u001b[0;32m    148\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    149\u001b[0m     source: Union[\u001b[38;5;28mstr\u001b[39m, Path, \u001b[38;5;28mint\u001b[39m, Image\u001b[38;5;241m.\u001b[39mImage, \u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mtuple\u001b[39m, np\u001b[38;5;241m.\u001b[39mndarray, torch\u001b[38;5;241m.\u001b[39mTensor] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    150\u001b[0m     stream: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    151\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    152\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mlist\u001b[39m:\n\u001b[0;32m    153\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    154\u001b[0m \u001b[38;5;124;03m    Alias for the predict method, enabling the model instance to be callable for predictions.\u001b[39;00m\n\u001b[0;32m    155\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    174\u001b[0m \u001b[38;5;124;03m        ...     print(f\"Detected {len(r)} objects in image\")\u001b[39;00m\n\u001b[0;32m    175\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 176\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43msource\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\Python\\Lib\\site-packages\\ultralytics\\engine\\model.py:554\u001b[0m, in \u001b[0;36mModel.predict\u001b[1;34m(self, source, stream, predictor, **kwargs)\u001b[0m\n\u001b[0;32m    552\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m prompts \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredictor, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mset_prompts\u001b[39m\u001b[38;5;124m\"\u001b[39m):  \u001b[38;5;66;03m# for SAM-type models\u001b[39;00m\n\u001b[0;32m    553\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredictor\u001b[38;5;241m.\u001b[39mset_prompts(prompts)\n\u001b[1;32m--> 554\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredictor\u001b[38;5;241m.\u001b[39mpredict_cli(source\u001b[38;5;241m=\u001b[39msource) \u001b[38;5;28;01mif\u001b[39;00m is_cli \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredictor\u001b[49m\u001b[43m(\u001b[49m\u001b[43msource\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msource\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\Python\\Lib\\site-packages\\ultralytics\\engine\\predictor.py:168\u001b[0m, in \u001b[0;36mBasePredictor.__call__\u001b[1;34m(self, source, model, stream, *args, **kwargs)\u001b[0m\n\u001b[0;32m    166\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstream_inference(source, model, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    167\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 168\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstream_inference(source, model, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs))\n",
      "File \u001b[1;32md:\\Python\\Lib\\site-packages\\torch\\utils\\_contextlib.py:36\u001b[0m, in \u001b[0;36m_wrap_generator.<locals>.generator_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     33\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     34\u001b[0m     \u001b[38;5;66;03m# Issuing `None` to a generator fires it up\u001b[39;00m\n\u001b[0;32m     35\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[1;32m---> 36\u001b[0m         response \u001b[38;5;241m=\u001b[39m gen\u001b[38;5;241m.\u001b[39msend(\u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m     38\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m     39\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     40\u001b[0m             \u001b[38;5;66;03m# Forward the response to our caller and get its next request\u001b[39;00m\n",
      "File \u001b[1;32md:\\Python\\Lib\\site-packages\\ultralytics\\engine\\predictor.py:254\u001b[0m, in \u001b[0;36mBasePredictor.stream_inference\u001b[1;34m(self, source, model, *args, **kwargs)\u001b[0m\n\u001b[0;32m    252\u001b[0m \u001b[38;5;66;03m# Inference\u001b[39;00m\n\u001b[0;32m    253\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m profilers[\u001b[38;5;241m1\u001b[39m]:\n\u001b[1;32m--> 254\u001b[0m     preds \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minference\u001b[49m\u001b[43m(\u001b[49m\u001b[43mim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    255\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39membed:\n\u001b[0;32m    256\u001b[0m         \u001b[38;5;28;01myield from\u001b[39;00m [preds] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(preds, torch\u001b[38;5;241m.\u001b[39mTensor) \u001b[38;5;28;01melse\u001b[39;00m preds  \u001b[38;5;66;03m# yield embedding tensors\u001b[39;00m\n",
      "File \u001b[1;32md:\\Python\\Lib\\site-packages\\ultralytics\\engine\\predictor.py:142\u001b[0m, in \u001b[0;36mBasePredictor.inference\u001b[1;34m(self, im, *args, **kwargs)\u001b[0m\n\u001b[0;32m    136\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Runs inference on a given image using the specified model and arguments.\"\"\"\u001b[39;00m\n\u001b[0;32m    137\u001b[0m visualize \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    138\u001b[0m     increment_path(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msave_dir \u001b[38;5;241m/\u001b[39m Path(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m])\u001b[38;5;241m.\u001b[39mstem, mkdir\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m    139\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mvisualize \u001b[38;5;129;01mand\u001b[39;00m (\u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msource_type\u001b[38;5;241m.\u001b[39mtensor)\n\u001b[0;32m    140\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m    141\u001b[0m )\n\u001b[1;32m--> 142\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maugment\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maugment\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvisualize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvisualize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membed\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\Python\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\Python\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32md:\\Python\\Lib\\site-packages\\ultralytics\\nn\\autobackend.py:461\u001b[0m, in \u001b[0;36mAutoBackend.forward\u001b[1;34m(self, im, augment, visualize, embed)\u001b[0m\n\u001b[0;32m    459\u001b[0m \u001b[38;5;66;03m# PyTorch\u001b[39;00m\n\u001b[0;32m    460\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpt \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnn_module:\n\u001b[1;32m--> 461\u001b[0m     y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maugment\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maugment\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvisualize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvisualize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43membed\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    463\u001b[0m \u001b[38;5;66;03m# TorchScript\u001b[39;00m\n\u001b[0;32m    464\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mjit:\n",
      "File \u001b[1;32md:\\Python\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\Python\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32md:\\Python\\Lib\\site-packages\\ultralytics\\nn\\tasks.py:112\u001b[0m, in \u001b[0;36mBaseModel.forward\u001b[1;34m(self, x, *args, **kwargs)\u001b[0m\n\u001b[0;32m    110\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mdict\u001b[39m):  \u001b[38;5;66;03m# for cases of training and validating while training.\u001b[39;00m\n\u001b[0;32m    111\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloss(x, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m--> 112\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\Python\\Lib\\site-packages\\ultralytics\\nn\\tasks.py:130\u001b[0m, in \u001b[0;36mBaseModel.predict\u001b[1;34m(self, x, profile, visualize, augment, embed)\u001b[0m\n\u001b[0;32m    128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m augment:\n\u001b[0;32m    129\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_predict_augment(x)\n\u001b[1;32m--> 130\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_predict_once\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprofile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvisualize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membed\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\Python\\Lib\\site-packages\\ultralytics\\nn\\tasks.py:151\u001b[0m, in \u001b[0;36mBaseModel._predict_once\u001b[1;34m(self, x, profile, visualize, embed)\u001b[0m\n\u001b[0;32m    149\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m profile:\n\u001b[0;32m    150\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_profile_one_layer(m, x, dt)\n\u001b[1;32m--> 151\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[43mm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# run\u001b[39;00m\n\u001b[0;32m    152\u001b[0m y\u001b[38;5;241m.\u001b[39mappend(x \u001b[38;5;28;01mif\u001b[39;00m m\u001b[38;5;241m.\u001b[39mi \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msave \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)  \u001b[38;5;66;03m# save output\u001b[39;00m\n\u001b[0;32m    153\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m visualize:\n",
      "File \u001b[1;32md:\\Python\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\Python\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32md:\\Python\\Lib\\site-packages\\ultralytics\\nn\\modules\\head.py:67\u001b[0m, in \u001b[0;36mDetect.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     65\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining:  \u001b[38;5;66;03m# Training path\u001b[39;00m\n\u001b[0;32m     66\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x\n\u001b[1;32m---> 67\u001b[0m y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inference\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     68\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m y \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexport \u001b[38;5;28;01melse\u001b[39;00m (y, x)\n",
      "File \u001b[1;32md:\\Python\\Lib\\site-packages\\ultralytics\\nn\\modules\\head.py:100\u001b[0m, in \u001b[0;36mDetect._inference\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     98\u001b[0m x_cat \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat([xi\u001b[38;5;241m.\u001b[39mview(shape[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mno, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m xi \u001b[38;5;129;01min\u001b[39;00m x], \u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m     99\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdynamic \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mshape \u001b[38;5;241m!=\u001b[39m shape:\n\u001b[1;32m--> 100\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39manchors, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstrides \u001b[38;5;241m=\u001b[39m (x\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m \u001b[43mmake_anchors\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.5\u001b[39;49m\u001b[43m)\u001b[49m)\n\u001b[0;32m    101\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mshape \u001b[38;5;241m=\u001b[39m shape\n\u001b[0;32m    103\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexport \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mformat \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msaved_model\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpb\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtflite\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124medgetpu\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtfjs\u001b[39m\u001b[38;5;124m\"\u001b[39m}:  \u001b[38;5;66;03m# avoid TF FlexSplitV ops\u001b[39;00m\n",
      "File \u001b[1;32md:\\Python\\Lib\\site-packages\\ultralytics\\utils\\tal.py:313\u001b[0m, in \u001b[0;36mmake_anchors\u001b[1;34m(feats, strides, grid_cell_offset)\u001b[0m\n\u001b[0;32m    311\u001b[0m     sy \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39marange(end\u001b[38;5;241m=\u001b[39mh, device\u001b[38;5;241m=\u001b[39mdevice, dtype\u001b[38;5;241m=\u001b[39mdtype) \u001b[38;5;241m+\u001b[39m grid_cell_offset  \u001b[38;5;66;03m# shift y\u001b[39;00m\n\u001b[0;32m    312\u001b[0m     sy, sx \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmeshgrid(sy, sx, indexing\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mij\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mif\u001b[39;00m TORCH_1_10 \u001b[38;5;28;01melse\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mmeshgrid(sy, sx)\n\u001b[1;32m--> 313\u001b[0m     anchor_points\u001b[38;5;241m.\u001b[39mappend(\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstack\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43msx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msy\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m))\n\u001b[0;32m    314\u001b[0m     stride_tensor\u001b[38;5;241m.\u001b[39mappend(torch\u001b[38;5;241m.\u001b[39mfull((h \u001b[38;5;241m*\u001b[39m w, \u001b[38;5;241m1\u001b[39m), stride, dtype\u001b[38;5;241m=\u001b[39mdtype, device\u001b[38;5;241m=\u001b[39mdevice))\n\u001b[0;32m    315\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcat(anchor_points), torch\u001b[38;5;241m.\u001b[39mcat(stride_tensor)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "from ultralytics import YOLO\n",
    "from PIL import Image\n",
    "\n",
    "def create_folder(directory):\n",
    "    if not os.path.exists(directory):\n",
    "        os.makedirs(directory)\n",
    "\n",
    "def get_all_image_paths(folder_path):\n",
    "    image_extensions = ['.jpg', '.jpeg', '.png', '.bmp', '.tiff']\n",
    "    all_image_paths = []\n",
    "    for root, _, files in os.walk(folder_path):\n",
    "        for file in files:\n",
    "            if any(file.lower().endswith(ext) for ext in image_extensions):\n",
    "                all_image_paths.append(os.path.join(root, file))\n",
    "    return all_image_paths\n",
    "\n",
    "def iou(box1, box2):\n",
    "    \"\"\"计算两个边界框之间的交并比（IoU）\"\"\"\n",
    "    x1, y1, x2, y2 = box1\n",
    "    x3, y3, x4, y4 = box2\n",
    "    inter_x1 = max(x1, x3)\n",
    "    inter_y1 = max(y1, y3)\n",
    "    inter_x2 = min(x2, x4)\n",
    "    inter_y2 = min(y2, y4)\n",
    "    inter_area = max(0, inter_x2 - inter_x1) * max(0, inter_y2 - inter_y1)\n",
    "    box1_area = (x2 - x1) * (y2 - y1)\n",
    "    box2_area = (x4 - x3) * (y4 - y3)\n",
    "    union_area = box1_area + box2_area - inter_area\n",
    "    return inter_area / union_area if union_area != 0 else 0\n",
    "\n",
    "def save_boxes_to_file(boxes, file_path):\n",
    "    \"\"\"将边界框参数保存到文件\"\"\"\n",
    "    with open(file_path, 'w') as f:\n",
    "        for box in boxes:\n",
    "            f.write(','.join(map(str, box)) + '\\n')\n",
    "\n",
    "def detect_and_classify_images(base_path, output_base_path):\n",
    "    yolo_model_person = YOLO('yolov8m.pt')  # 使用预训练的YOLOv8模型\n",
    "    clothing_model = YOLO('deepfashion2_yolov8s-seg.pt')  # 衣服检测模型\n",
    "\n",
    "    for folder_name in os.listdir(base_path):\n",
    "        folder_path = os.path.join(base_path, folder_name)\n",
    "        if not os.path.isdir(folder_path):\n",
    "            continue\n",
    "\n",
    "        flat_clothing_dir = os.path.join(output_base_path, folder_name, '平铺服装')\n",
    "        human_clothing_dir = os.path.join(output_base_path, folder_name, '人身服装')\n",
    "        not_qualify_dir = os.path.join(output_base_path, folder_name, '不满足要求')\n",
    "\n",
    "        create_folder(flat_clothing_dir)\n",
    "        create_folder(human_clothing_dir)\n",
    "        create_folder(not_qualify_dir)\n",
    "\n",
    "        image_paths = get_all_image_paths(folder_path)\n",
    "\n",
    "        for image_path in image_paths:\n",
    "            image = Image.open(image_path)\n",
    "            image_area = image.size[0] * image.size[1]\n",
    "\n",
    "            print(f\"\\nProcessing image: {os.path.basename(image_path)}\")\n",
    "\n",
    "            # Step 1: 检测人\n",
    "            results_person = yolo_model_person(image_path, classes=[0], device='cuda')\n",
    "            person_boxes = [box.xyxy.tolist()[0] for result in results_person for box in result.boxes]\n",
    "\n",
    "            if person_boxes:\n",
    "                largest_person_box = max(person_boxes, key=lambda box: (box[2] - box[0]) * (box[3] - box[1]))\n",
    "                person_area = (largest_person_box[2] - largest_person_box[0]) * (largest_person_box[3] - largest_person_box[1])\n",
    "\n",
    "                person_area_ratio = person_area / image_area\n",
    "                print(f\"Detected person boxes: {person_boxes}\")\n",
    "                print(f\"Largest person box area ratio: {person_area_ratio:.2f}\")\n",
    "\n",
    "                iou_values = [iou(largest_person_box, other_box) for other_box in person_boxes if other_box != largest_person_box]\n",
    "                print(f\"Person IoUs with other boxes: {iou_values}\")\n",
    "\n",
    "                if person_area_ratio > 0.5 and all(value < 0.1 for value in iou_values):\n",
    "                    image_name = os.path.basename(image_path)\n",
    "                    shutil.copy(image_path, os.path.join(human_clothing_dir, image_name))\n",
    "                    save_boxes_to_file([largest_person_box], os.path.join(human_clothing_dir, image_name.replace('.jpg', '_boxes.txt')))\n",
    "                    print(\"Classified as human clothing\")\n",
    "                    continue\n",
    "\n",
    "            # Step 2: 检测衣服\n",
    "            results_clothing = clothing_model(image_path, device='cuda')\n",
    "            clothing_boxes = [box.xyxy.tolist()[0] for result in results_clothing for box in result.boxes]\n",
    "\n",
    "            if clothing_boxes:\n",
    "                largest_clothing_box = max(clothing_boxes, key=lambda box: (box[2] - box[0]) * (box[3] - box[1]))\n",
    "                clothing_area = (largest_clothing_box[2] - largest_clothing_box[0]) * (largest_clothing_box[3] - largest_clothing_box[1])\n",
    "\n",
    "                clothing_area_ratio = clothing_area / image_area\n",
    "                print(f\"Detected clothing boxes: {clothing_boxes}\")\n",
    "                print(f\"Largest clothing box area ratio: {clothing_area_ratio:.2f}\")\n",
    "\n",
    "                iou_values_clothing = [iou(largest_clothing_box, other_box) for other_box in clothing_boxes if other_box != largest_clothing_box]\n",
    "                print(f\"Clothing IoUs with other boxes: {iou_values_clothing}\")\n",
    "\n",
    "                if clothing_area_ratio > 0.5 and all(value < 0.1 for value in iou_values_clothing):\n",
    "                    image_name = os.path.basename(image_path)\n",
    "                    shutil.copy(image_path, os.path.join(flat_clothing_dir, image_name))\n",
    "                    save_boxes_to_file([largest_clothing_box], os.path.join(flat_clothing_dir, image_name.replace('.jpg', '_boxes.txt')))\n",
    "                    print(\"Classified as flat clothing\")\n",
    "                    continue\n",
    "\n",
    "            # 如果都不符合\n",
    "            image_name = os.path.basename(image_path)\n",
    "            shutil.copy(image_path, os.path.join(not_qualify_dir, image_name))\n",
    "            print(\"Classified as not qualified\")\n",
    "\n",
    "# 使用示例路径，请根据实际情况调整\n",
    "base_path = '夹克'\n",
    "output_base_path = '夹克1'\n",
    "detect_and_classify_images(base_path, output_base_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
